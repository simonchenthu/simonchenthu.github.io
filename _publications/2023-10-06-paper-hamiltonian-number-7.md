---
title: "Accelerating Optimization over the Space of Probability Measures"
collection: publications
permalink: /publication/2023-10-06-paper-hamiltonian-number-7
excerpt: #'This paper is about the number 1. The number 2 is left for future work.'
date: 2023-10-06
pubdate: 2023-10-06
venuestatus: 'Submitted to'
venue: 'Journal of Machine Learning Research'
cauthor: 'Qin Li, Oliver Tse and Leonardo Zepeda-Núñez'
# paperurl: 'https://doi.org/10.1137/22M147075X'
arxivurl: 'https://arxiv.org/abs/2310.04006'
# posterurl: '/files/helmholtz-poster.pdf'
# codeurl:  'https://github.com/simonchenthu/inverse_scattering/tree/husimi'
citation: #'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
imgurl: /images/hamiltonian_summary.png
imgcap: Convergence rates of three momentum-based methods. All the methods share the same convergence rate to optimize a finite-dimensional function f(x) and a functional E[ρ].
imgfloat: right
imgwidth: 500px
---
Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order.